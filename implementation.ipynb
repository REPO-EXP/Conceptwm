{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from utils.misc import torch_to_pil\n",
    "import gc\n",
    "env = os.environ.copy()\n",
    "env['MODEL_PATH'] = '../../dreambooth-outputs//wm_concept_implementation_person/checkpoint-49'\n",
    "env['CLASS_DIR'] = 'data/class-person'\n",
    "env['DREAMBOOTH_OUTPUT_DIR'] = '../../dreambooth-outputs/wm_concept_implementation_person/'\n",
    "env['INSTANCE_DIR_Train'] = \"CelebA-HQ/44/set_A\"\n",
    "env['INSTANCE_DIR_Adversarial'] = \"CelebA-HQ/44/set_B\"\n",
    "env['WM_INSTANCE_DIR'] = \"CelebA-HQ/44/set_A_W\"\n",
    "input_args = [\n",
    "    '--pretrained_model_name_or_path', f\"{env['MODEL_PATH']}\",\n",
    "    '--enable_xformers_memory_efficient_attention',\n",
    "    '--instance_data_dir_for_train',f\"{env['INSTANCE_DIR_Train']}\",\n",
    "    '--wm_instance_data_dir_for_adversarial',f\"{env['WM_INSTANCE_DIR']}\",\n",
    "    '--instance_data_dir_for_adversarial',f\"{env['INSTANCE_DIR_Adversarial']}\",\n",
    "    '--class_data_dir', f\"{env['CLASS_DIR']}\",\n",
    "    '--output_dir', f\"{env['DREAMBOOTH_OUTPUT_DIR']}\",\n",
    "    '--with_prior_preservation',\n",
    "    '--prior_loss_weight', '1.0',\n",
    "    '--instance_prompt', 'a photo of <concept1> person',\n",
    "    '--class_prompt', 'a photo of person',\n",
    "    '--inference_prompt', 'a photo of <concept1> person',\n",
    "    '--resolution', '512',\n",
    "    '--train_batch_size', '1',\n",
    "    '--gradient_accumulation_steps', '1',\n",
    "    '--learning_rate', '5e-6',\n",
    "    '--lr_scheduler', 'constant',\n",
    "    '--lr_warmup_steps', '0',\n",
    "    '--num_class_images', '200',\n",
    "    '--max_train_steps', '500',\n",
    "    '--checkpointing_iterations', '50',\n",
    "    '--center_crop',\n",
    "    '--mixed_precision', 'bf16',\n",
    "    '--prior_generation_precision', 'bf16',\n",
    "    '--sample_batch_size', '8',\n",
    "    '--gradient_checkpointing',\n",
    "    '--use_8bit_adam'\n",
    "    '--max_f_train_steps','4',\n",
    "    '--max_adv_train_steps','1'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import hashlib\n",
    "import itertools\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import diffusers\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, UNet2DConditionModel\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, PretrainedConfig\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, StableDiffusionPipeline, UNet2DConditionModel\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "image_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "def infer(checkpoint_path, prompts=None, n_img=16, bs=8, n_steps=100, guidance_scale=7.5,sec_decoder=None,msg_val=None):\n",
    "    import gc\n",
    "    with torch.no_grad():\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            checkpoint_path, safety_checker=None,torch_dtype=torch.float16\n",
    "        ).to(\"cuda\")\n",
    "        pipe.enable_xformers_memory_efficient_attention()\n",
    "        torch.cuda.empty_cache()\n",
    "        for prompt in prompts:\n",
    "            print(prompt)\n",
    "            norm_prompt = prompt.lower().replace(\",\", \"\").replace(\" \", \"_\")\n",
    "            out_path = f\"{checkpoint_path}/dreambooth/{norm_prompt}\"\n",
    "            os.makedirs(out_path, exist_ok=True)\n",
    "            for i in range(n_img // bs):\n",
    "                images = pipe(\n",
    "                    [prompt] * bs,\n",
    "                    num_inference_steps=n_steps,\n",
    "                    guidance_scale=guidance_scale,\n",
    "                ).images\n",
    "                for idx, image in enumerate(images):\n",
    "                    image.save(f\"{out_path}/{i}_{idx}.png\")\n",
    "                    \n",
    "                    weight_dtype = next(sec_decoder.parameters()).dtype\n",
    "                    device = next(sec_decoder.parameters()).device\n",
    "                    image=image_transforms(image).to(dtype=weight_dtype,device=device)\n",
    "                    \n",
    "                    print(image.shape)\n",
    "                    watermarked_image_pil = torch_to_pil(image)[0]\n",
    "                    watermarked_image_pil.show()\n",
    "                    decoded_msg = sec_decoder(image.unsqueeze(0))\n",
    "                    decoded_msg = torch.argmax(decoded_msg, dim=2)\n",
    "                    acc = 1 - torch.abs(decoded_msg - msg_val).sum().float() / (48 * 1)\n",
    "                    print(f\"acc {acc}\")\n",
    "                    \n",
    "                    torch.cuda.empty_cache()\n",
    "        del pipe  \n",
    "        torch.cuda.empty_cache()  \n",
    "        gc.collect()  \n",
    "\n",
    "class DreamBoothDatasetFromTensor(Dataset):\n",
    "    \"\"\"Just like DreamBoothDataset, but take instance_images_tensor instead of path\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        instance_images_tensor,\n",
    "        instance_prompt,\n",
    "        tokenizer,\n",
    "        class_data_root=None,\n",
    "        class_prompt=None,\n",
    "        size=512,\n",
    "        center_crop=False,\n",
    "    ):\n",
    "        self.size = size\n",
    "        self.center_crop = center_crop\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.instance_images_tensor = instance_images_tensor\n",
    "        self.num_instance_images = len(self.instance_images_tensor)\n",
    "        self.instance_prompt = instance_prompt\n",
    "        self._length = self.num_instance_images\n",
    "\n",
    "        if class_data_root is not None:\n",
    "            self.class_data_root = Path(class_data_root)\n",
    "            self.class_data_root.mkdir(parents=True, exist_ok=True)\n",
    "            self.class_images_path = list(self.class_data_root.iterdir())\n",
    "            self.num_class_images = len(self.class_images_path)\n",
    "            self._length = max(self.num_class_images, self.num_instance_images)\n",
    "            self.class_prompt = class_prompt\n",
    "        else:\n",
    "            self.class_data_root = None\n",
    "\n",
    "        self.image_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        instance_image = self.instance_images_tensor[index % self.num_instance_images]\n",
    "        example[\"instance_images\"] = instance_image\n",
    "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
    "            self.instance_prompt,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids\n",
    "\n",
    "        if self.class_data_root:\n",
    "            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n",
    "            if not class_image.mode == \"RGB\":\n",
    "                class_image = class_image.convert(\"RGB\")\n",
    "            example[\"class_images\"] = self.image_transforms(class_image)\n",
    "            example[\"class_prompt_ids\"] = self.tokenizer(\n",
    "                self.class_prompt,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.tokenizer.model_max_length,\n",
    "                return_tensors=\"pt\",\n",
    "            ).input_ids\n",
    "\n",
    "        return example\n",
    "\n",
    "\n",
    "class DreamBoothDatasetFromPriorTensor(Dataset):\n",
    "    \"\"\"Just like DreamBoothDataset, but take instance_images_tensor instead of path\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        instance_images_tensor,\n",
    "        wm_data_tensor,\n",
    "        instance_prompt,\n",
    "        tokenizer,\n",
    "        class_data_root=None,\n",
    "        class_prompt=None,\n",
    "        size=512,\n",
    "        center_crop=False,\n",
    "    ):\n",
    "        self.size = size\n",
    "        self.center_crop = center_crop\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.wm_instance_images_tensor = instance_images_tensor\n",
    "        self.num_wm_instance_images = len(self.wm_instance_images_tensor)\n",
    "        \n",
    "        self.instance_images_tensor = instance_images_tensor\n",
    "        self.num_instance_images = len(self.instance_images_tensor)\n",
    "        self.instance_prompt = instance_prompt\n",
    "        self._length = self.num_instance_images\n",
    "\n",
    "        if class_data_root is not None:\n",
    "            self.class_data_root = Path(class_data_root)\n",
    "            self.class_data_root.mkdir(parents=True, exist_ok=True)\n",
    "            self.class_images_path = list(self.class_data_root.iterdir())\n",
    "            self.num_class_images = len(self.class_images_path)\n",
    "            self._length = max(self.num_class_images, self.num_instance_images)\n",
    "            self.class_prompt = class_prompt\n",
    "        else:\n",
    "            self.class_data_root = None\n",
    "\n",
    "        self.image_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        instance_image = self.instance_images_tensor[index % self.num_instance_images]\n",
    "        wm_instance_image = self.wm_instance_images_tensor[index % self.num_wm_instance_images]\n",
    "        example[\"wm_instance_images\"] = instance_image\n",
    "        example[\"instance_images\"] = instance_image\n",
    "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
    "            self.instance_prompt,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids\n",
    "\n",
    "        if self.class_data_root:\n",
    "            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n",
    "            if not class_image.mode == \"RGB\":\n",
    "                class_image = class_image.convert(\"RGB\")\n",
    "            example[\"class_images\"] = self.image_transforms(class_image)\n",
    "            example[\"class_prompt_ids\"] = self.tokenizer(\n",
    "                self.class_prompt,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.tokenizer.model_max_length,\n",
    "                return_tensors=\"pt\",\n",
    "            ).input_ids\n",
    "\n",
    "        return example\n",
    "\n",
    "def import_model_class_from_model_name_or_path(pretrained_model_name_or_path: str, revision: str):\n",
    "    text_encoder_config = PretrainedConfig.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"text_encoder\",\n",
    "        revision=revision,\n",
    "    )\n",
    "    model_class = text_encoder_config.architectures[0]\n",
    "\n",
    "    if model_class == \"CLIPTextModel\":\n",
    "        from transformers import CLIPTextModel\n",
    "\n",
    "        return CLIPTextModel\n",
    "    elif model_class == \"RobertaSeriesModelWithTransformation\":\n",
    "        from diffusers.pipelines.alt_diffusion.modeling_roberta_series import RobertaSeriesModelWithTransformation\n",
    "\n",
    "        return RobertaSeriesModelWithTransformation\n",
    "    else:\n",
    "        raise ValueError(f\"{model_class} is not supported.\")\n",
    "\n",
    "\n",
    "def parse_args(input_args=None):\n",
    "    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n",
    "    parser.add_argument(\n",
    "        \"--inference_prompts\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"The prompt used to generate images at inference.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pretrained_model_name_or_path\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=True,\n",
    "        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--revision\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=False,\n",
    "        help=(\n",
    "            \"Revision of pretrained model identifier from huggingface.co/models. Trainable model components should be\"\n",
    "            \" float32 precision.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "    parser.add_argument(\n",
    "        \"--tokenizer_name\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--instance_data_dir_for_train\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=True,\n",
    "        help=\"A folder containing the training data of instance images.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--instance_data_dir_for_adversarial\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=True,\n",
    "        help=\"A folder containing the images to add adversarial noise\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--wm_instance_data_dir_for_adversarial\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=True,\n",
    "        help=\"A folder containing the images to add adversarial noise\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--class_data_dir\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=False,\n",
    "        help=\"A folder containing the training data of class images.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--instance_prompt\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=True,\n",
    "        help=\"The prompt with identifier specifying the instance\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--class_prompt\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"The prompt to specify images in the same class as provided instance images.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--with_prior_preservation\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Flag to add prior preservation loss.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--prior_loss_weight\",\n",
    "        type=float,\n",
    "        default=1.0,\n",
    "        help=\"The weight of prior preservation loss.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_class_images\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "        help=(\n",
    "            \"Minimal class images for prior preservation loss. If there are not enough images already present in\"\n",
    "            \" class_data_dir, additional images will be sampled with class_prompt.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        type=str,\n",
    "        default=\"text-inversion-model\",\n",
    "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", type=int, default=None, help=\"A seed for reproducible training.\")\n",
    "    parser.add_argument(\n",
    "        \"--resolution\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=(\n",
    "            \"The resolution for input images, all the images in the train/validation dataset will be resized to this\"\n",
    "            \" resolution\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--center_crop\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=(\n",
    "            \"Whether to center crop the input images to the resolution. If not set, the images will be randomly\"\n",
    "            \" cropped. The images will be resized to the resolution first before cropping.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_text_encoder\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to train the text encoder. If set, the text encoder should be float32 precision.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_batch_size\",\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help=\"Batch size (per device) for the training dataloader.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sample_batch_size\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"Batch size (per device) for sampling images.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_train_steps\",\n",
    "        type=int,\n",
    "        default=20,\n",
    "        help=\"Total number of training steps to perform.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_f_train_steps\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help=\"Total number of sub-steps to train surogate model.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_adv_train_steps\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help=\"Total number of sub-steps to train adversarial noise.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--checkpointing_iterations\",\n",
    "        type=int,\n",
    "        default=5,\n",
    "        help=(\"Save a checkpoint of the training state every X iterations.\"),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        type=float,\n",
    "        default=5e-6,\n",
    "        help=\"Initial learning rate (after the potential warmup period) to use.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--logging_dir\",\n",
    "        type=str,\n",
    "        default=\"logs\",\n",
    "        help=(\n",
    "            \"[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\"\n",
    "            \" *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_checkpointing\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_8bit_adam\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether or not to use 8-bit Adam from bitsandbytes.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--allow_tf32\",\n",
    "        action=\"store_true\",\n",
    "        help=(\n",
    "            \"Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see\"\n",
    "            \" https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--report_to\",\n",
    "        type=str,\n",
    "        default=\"tensorboard\",\n",
    "        help=(\n",
    "            'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`'\n",
    "            ' (default), `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` to report to all integrations.'\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mixed_precision\",\n",
    "        type=str,\n",
    "        default=\"fp16\",\n",
    "        choices=[\"no\", \"fp16\", \"bf16\"],\n",
    "        help=(\n",
    "            \"Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >=\"\n",
    "            \" 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the\"\n",
    "            \" flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--enable_xformers_memory_efficient_attention\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether or not to use xformers.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pgd_alpha\",\n",
    "        type=float,\n",
    "        default=0.1 / 255,\n",
    "        help=\"The step size for pgd.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pgd_eps\",\n",
    "        type=float,\n",
    "        default=0.0001,\n",
    "        help=\"The noise budget for pgd.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--target_image_path\",\n",
    "        default=None,\n",
    "        help=\"target image for attacking\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--adam_beta1\",\n",
    "        type=float,\n",
    "        default=0.9,\n",
    "        help=\"The beta1 parameter for the Adam optimizer.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--adam_beta2\",\n",
    "        type=float,\n",
    "        default=0.999,\n",
    "        help=\"The beta2 parameter for the Adam optimizer.\",\n",
    "    )\n",
    "    parser.add_argument(\"--adam_weight_decay\", type=float, default=1e-2, help=\"Weight decay to use.\")\n",
    "    parser.add_argument(\n",
    "        \"--adam_epsilon\",\n",
    "        type=float,\n",
    "        default=1e-08,\n",
    "        help=\"Epsilon value for the Adam optimizer\",\n",
    "    )\n",
    "\n",
    "    if input_args is not None:\n",
    "        args = parser.parse_known_args(input_args)[0]\n",
    "    else:\n",
    "        args = parser.parse_known_args()[0]\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptDataset(Dataset):\n",
    "    \"A simple dataset to prepare the prompts to generate class images on multiple GPUs.\"\n",
    "\n",
    "    def __init__(self, prompt, num_samples):\n",
    "        self.prompt = prompt\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        example[\"prompt\"] = self.prompt\n",
    "        example[\"index\"] = index\n",
    "        return example\n",
    "\n",
    "\n",
    "def load_data(data_dir, size=512, center_crop=True) -> torch.Tensor:\n",
    "    image_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # images = [image_transforms(Image.open(i).convert(\"RGB\")) for i in list(Path(data_dir).iterdir())]\n",
    "    images = [image_transforms(Image.open(i).convert(\"RGB\")) for i in sorted(Path(data_dir).iterdir(), key=lambda x: int(x.stem))]\n",
    "    images = torch.stack(images)\n",
    "    return images\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    args,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    noise_scheduler,\n",
    "    vae,\n",
    "    text_encoder,\n",
    "    data_tensor: torch.Tensor,\n",
    "    num_steps=20,\n",
    "):\n",
    "    # Load the tokenizer\n",
    "    \n",
    "    # unet, text_encoder = copy.deepcopy(models[0]), copy.deepcopy(models[1])\n",
    "    unet=copy.deepcopy(model[0])\n",
    "    unet.requires_grad_(True)\n",
    "    # params_to_optimize = itertools.chain(unet.parameters(), text_encoder.parameters())\n",
    "    params_to_optimize = itertools.chain(unet.parameters())\n",
    "\n",
    "    if args.use_8bit_adam:\n",
    "        try:\n",
    "            import bitsandbytes as bnb\n",
    "            optimizer_class = bnb.optim.AdamW8bit\n",
    "        except ImportError:\n",
    "            print(\"bitsandbytes not installed. Falling back to default Adam.\")\n",
    "            optimizer_class = torch.optim.AdamW\n",
    "    else:\n",
    "        optimizer_class = torch.optim.AdamW\n",
    "    optimizer = optimizer_class(\n",
    "        params_to_optimize,\n",
    "        lr=args.learning_rate,\n",
    "        betas=(args.adam_beta1, args.adam_beta2),\n",
    "        weight_decay=args.adam_weight_decay,\n",
    "        eps=args.adam_epsilon,\n",
    "    )\n",
    "\n",
    "    train_dataset = DreamBoothDatasetFromTensor(\n",
    "        data_tensor,\n",
    "        args.instance_prompt,\n",
    "        tokenizer,\n",
    "        args.class_data_dir,\n",
    "        args.class_prompt,\n",
    "        args.resolution,\n",
    "        args.center_crop,\n",
    "    )\n",
    "\n",
    "    weight_dtype = torch.bfloat16\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    vae.to(device, dtype=weight_dtype)\n",
    "    text_encoder.to(device, dtype=weight_dtype)\n",
    "    unet.to(device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    for step in (range(num_steps)):\n",
    "        unet.train()\n",
    "        \n",
    "        from utils.misc import torch_to_pil\n",
    "        \n",
    "        step_data = train_dataset[step % len(train_dataset)]\n",
    "        pixel_values = torch.stack([step_data[\"instance_images\"], step_data[\"class_images\"]]).to(\n",
    "            device, dtype=weight_dtype\n",
    "        )\n",
    "        # print(pixel_values.shape)\n",
    "        # torch_to_pil(pixel_values)[0].show()\n",
    "        # print(step_data[\"instance_prompt_ids\"], step_data[\"class_prompt_ids\"])\n",
    "        \n",
    "        input_ids = torch.cat([step_data[\"instance_prompt_ids\"], step_data[\"class_prompt_ids\"]], dim=0).to(device)\n",
    "\n",
    "        latents = vae.encode(pixel_values).latent_dist.sample()\n",
    "        latents = latents * vae.config.scaling_factor\n",
    "\n",
    "        # Sample noise that we'll add to the latents\n",
    "        noise = torch.randn_like(latents)\n",
    "        bsz = latents.shape[0]\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "        timesteps = timesteps.long()\n",
    "\n",
    "        # Add noise to the latents according to the noise magnitude at each timestep\n",
    "        # (this is the forward diffusion process)\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        # Get the text embedding for conditioning\n",
    "        encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "\n",
    "        # Predict the noise residual\n",
    "        model_pred = unet(noisy_latents.to(unet.dtype), timesteps, encoder_hidden_states.to(unet.dtype)).sample\n",
    "\n",
    "        # Get the target for loss depending on the prediction type\n",
    "        if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "            target = noise\n",
    "        elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "            target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "        # with prior preservation loss\n",
    "        if args.with_prior_preservation:\n",
    "            model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)\n",
    "            target, target_prior = torch.chunk(target, 2, dim=0)\n",
    "\n",
    "            # Compute instance loss\n",
    "            instance_loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "\n",
    "            # Compute prior loss\n",
    "            prior_loss = F.mse_loss(model_pred_prior.float(), target_prior.float(), reduction=\"mean\")\n",
    "\n",
    "            # Add the prior loss to the instance loss.\n",
    "            loss = instance_loss + args.prior_loss_weight * prior_loss\n",
    "            # loss = instance_loss\n",
    "\n",
    "        else:\n",
    "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "        print(f'loss={loss}')\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(params_to_optimize, 1.0, error_if_nonfinite=True)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    logs = {\"loss\": loss.detach().item()}\n",
    "    \n",
    "    return [unet,logs]\n",
    "\n",
    "\n",
    "def perturbation_modualtion(\n",
    "    args,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    noise_scheduler,\n",
    "    vae,\n",
    "    text_encoder,\n",
    "    data_tensor: torch.Tensor,\n",
    "    original_images: torch.Tensor,\n",
    "    num_steps: int,\n",
    "    wm_tensor: torch.Tensor,\n",
    "    sec_decoder,\n",
    "    \n",
    "):\n",
    "    import numpy as np\n",
    "    def decode_latents(latents):\n",
    "        #latents = 1 / vae.config.scaling_factor * latents\n",
    "        image = vae.decode(latents).sample\n",
    "        return image\n",
    "    \"\"\"Return new perturbed data\"\"\"\n",
    "    \n",
    "    msg_val = torch.tensor(np.load('./secret_48.npy')).to('cuda')\n",
    "    unet= model[0]\n",
    "    weight_dtype = torch.bfloat16\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    unet.to(device, dtype=weight_dtype)\n",
    "    perturbed_images = data_tensor.detach().clone()\n",
    "    perturbed_images.to(torch.float32).requires_grad_(True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    import torch.optim as optim\n",
    "    optimizer = optim.Adam([perturbed_images], lr=1e-7)\n",
    "    for step in range(num_steps):\n",
    "        input_ids = tokenizer(\n",
    "        args.instance_prompt,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        return_tensors=\"pt\",\n",
    "        ).input_ids.repeat(len(data_tensor), 1)\n",
    "        latents = vae.encode(perturbed_images.to(device, dtype=weight_dtype)).latent_dist.sample()\n",
    "        latents = latents * vae.config.scaling_factor\n",
    "\n",
    "        # Sample noise that we'll add to the latents\n",
    "        noise = torch.randn_like(latents)\n",
    "        bsz = latents.shape[0]\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "        timesteps = timesteps.long()\n",
    "        # Add noise to the latents according to the noise magnitude at each timestep\n",
    "        # (this is the forward diffusion process)\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        # Get the text embedding for conditioning\n",
    "        encoder_hidden_states = text_encoder(input_ids.to(device))[0]\n",
    "\n",
    "        # Predict the noise residual\n",
    "        model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "        # Get the target for loss depending on the prediction type\n",
    "        if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "            target = noise\n",
    "        elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "            target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "        unet.zero_grad()\n",
    "        loss0 = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "        loss0.backward()\n",
    "\n",
    "        alpha = args.pgd_alpha\n",
    "        eps = args.pgd_eps\n",
    "        adv_images = perturbed_images + alpha * perturbed_images.grad.sign()\n",
    "        eta = torch.clamp(adv_images - wm_tensor, min=-eps, max=+eps)\n",
    "        perturbed_images=(wm_tensor+eta).detach_()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        perturbed_images.requires_grad_(True)\n",
    "        # change to BCE loss\n",
    "        image=perturbed_images.to(vae.dtype)\n",
    "        decoded_msg = sec_decoder(image.to(next(sec_decoder.parameters()).dtype).cuda())\n",
    "        labels = F.one_hot(msg_val.unsqueeze(0).expand(decoded_msg.shape[0],-1), num_classes=2).float()\n",
    "        msgloss = F.binary_cross_entropy_with_logits(decoded_msg, labels.cuda())\n",
    "        loss=msgloss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"PGD loss - step {step}, loss: {loss.detach().item()},loss0: {loss0.detach().item()},msg: {msgloss.detach().item()}\")\n",
    "        \n",
    "    perturbed_images.requires_grad_(False)\n",
    "    torch.cuda.empty_cache()   \n",
    "    return perturbed_images\n",
    "\n",
    "# def perturbation_modualtion(\n",
    "#     args,\n",
    "#     model,\n",
    "#     tokenizer,\n",
    "#     noise_scheduler,\n",
    "#     vae,\n",
    "#     text_encoder,\n",
    "#     data_tensor: torch.Tensor,\n",
    "#     original_images: torch.Tensor,\n",
    "#     target_tensor: torch.Tensor,\n",
    "#     num_steps: int,\n",
    "    \n",
    "# ):\n",
    "#     \"\"\"Return new perturbed data\"\"\"\n",
    "\n",
    "#     unet= model[0]\n",
    "#     weight_dtype = torch.bfloat16\n",
    "#     device = torch.device(\"cuda\")\n",
    "\n",
    "#     vae.to(device, dtype=weight_dtype)\n",
    "#     text_encoder.to(device, dtype=weight_dtype)\n",
    "#     unet.to(device, dtype=weight_dtype)\n",
    "\n",
    "#     perturbed_images = data_tensor.detach().clone()\n",
    "#     perturbed_images.to(torch.float32).requires_grad_(True)\n",
    "\n",
    "#     input_ids = tokenizer(\n",
    "#         args.instance_prompt,\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",\n",
    "#         max_length=tokenizer.model_max_length,\n",
    "#         return_tensors=\"pt\",\n",
    "#     ).input_ids.repeat(len(data_tensor), 1)\n",
    "\n",
    "#     for step in (range(num_steps)):\n",
    "#         perturbed_images.requires_grad = True\n",
    "#         latents = vae.encode(perturbed_images.to(device, dtype=weight_dtype)).latent_dist.sample()\n",
    "#         latents = latents * vae.config.scaling_factor\n",
    "\n",
    "#         # Sample noise that we'll add to the latents\n",
    "#         noise = torch.randn_like(latents)\n",
    "#         bsz = latents.shape[0]\n",
    "#         # Sample a random timestep for each image\n",
    "#         timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "#         timesteps = timesteps.long()\n",
    "#         # Add noise to the latents according to the noise magnitude at each timestep\n",
    "#         # (this is the forward diffusion process)\n",
    "#         noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "#         # Get the text embedding for conditioning\n",
    "#         encoder_hidden_states = text_encoder(input_ids.to(device))[0]\n",
    "\n",
    "#         # Predict the noise residual\n",
    "#         model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "#         # Get the target for loss depending on the prediction type\n",
    "#         if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "#             target = noise\n",
    "#         elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "#             target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "#         unet.zero_grad()\n",
    "#         text_encoder.zero_grad()\n",
    "#         loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "\n",
    "#         loss.backward()\n",
    "\n",
    "#         alpha = args.pgd_alpha\n",
    "#         eps = args.pgd_eps\n",
    "\n",
    "#         adv_images = perturbed_images + alpha * perturbed_images.grad.sign()\n",
    "#         eta = torch.clamp(adv_images - original_images, min=-eps, max=+eps)\n",
    "#         perturbed_images = torch.clamp(original_images + eta, min=-1, max=+1).detach_()\n",
    "\n",
    "#     return perturbed_images\n",
    "\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "\n",
    "args = parse_args(input_args)\n",
    "logging_dir = Path(args.output_dir, args.logging_dir)\n",
    "\n",
    "accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    mixed_precision=args.mixed_precision,\n",
    "    log_with=args.report_to,\n",
    "    project_config=accelerator_project_config,\n",
    ")\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(accelerator.state, main_process_only=False)\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_warning()\n",
    "    diffusers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "    diffusers.utils.logging.set_verbosity_error()\n",
    "\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)\n",
    "\n",
    "# Generate class images if prior preservation is enabled.\n",
    "if args.with_prior_preservation:\n",
    "    class_images_dir = Path(args.class_data_dir)\n",
    "    if not class_images_dir.exists():\n",
    "        class_images_dir.mkdir(parents=True)\n",
    "    cur_class_images = len(list(class_images_dir.iterdir()))\n",
    "\n",
    "    if cur_class_images < args.num_class_images:\n",
    "        torch_dtype = torch.float16 if accelerator.device.type == \"cuda\" else torch.float32\n",
    "        if args.mixed_precision == \"fp32\":\n",
    "            torch_dtype = torch.float32\n",
    "        elif args.mixed_precision == \"fp16\":\n",
    "            torch_dtype = torch.float16\n",
    "        elif args.mixed_precision == \"bf16\":\n",
    "            torch_dtype = torch.bfloat16\n",
    "        pipeline = DiffusionPipeline.from_pretrained(\n",
    "            args.pretrained_model_name_or_path,\n",
    "            torch_dtype=torch_dtype,\n",
    "            safety_checker=None,\n",
    "            revision=args.revision,\n",
    "        )\n",
    "        pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "        num_new_images = args.num_class_images - cur_class_images\n",
    "        logger.info(f\"Number of class images to sample: {num_new_images}.\")\n",
    "\n",
    "        sample_dataset = PromptDataset(args.class_prompt, num_new_images)\n",
    "        sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)\n",
    "\n",
    "        sample_dataloader = accelerator.prepare(sample_dataloader)\n",
    "        pipeline.to(accelerator.device)\n",
    "\n",
    "        for example in tqdm(\n",
    "            sample_dataloader,\n",
    "            desc=\"Generating class images\",\n",
    "            disable=not accelerator.is_local_main_process,\n",
    "        ):\n",
    "            images = pipeline(example[\"prompt\"]).images\n",
    "\n",
    "            for i, image in enumerate(images):\n",
    "                hash_image = hashlib.sha1(image.tobytes()).hexdigest()\n",
    "                image_filename = class_images_dir / f\"{example['index'][i] + cur_class_images}-{hash_image}.jpg\"\n",
    "                image.save(image_filename)\n",
    "\n",
    "        del pipeline\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# import correct text encoder class\n",
    "text_encoder_cls = import_model_class_from_model_name_or_path(args.pretrained_model_name_or_path, args.revision)\n",
    "\n",
    "# Load scheduler and models\n",
    "text_encoder = text_encoder_cls.from_pretrained(\n",
    "    args.pretrained_model_name_or_path,\n",
    "    subfolder=\"text_encoder\",\n",
    "    revision=args.revision,\n",
    ")\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.revision\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.pretrained_model_name_or_path,\n",
    "    subfolder=\"tokenizer\",\n",
    "    revision=args.revision,\n",
    "    use_fast=False,\n",
    ")\n",
    "\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision\n",
    ").cuda()\n",
    "\n",
    "vae.requires_grad_(False)\n",
    "\n",
    "if not args.train_text_encoder:\n",
    "    text_encoder.requires_grad_(False)\n",
    "\n",
    "if args.allow_tf32:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "clean_data = load_data(\n",
    "    args.instance_data_dir_for_train,\n",
    "    size=args.resolution,\n",
    "    center_crop=args.center_crop,\n",
    ")\n",
    "perturbed_data = load_data(\n",
    "    args.instance_data_dir_for_adversarial,\n",
    "    size=args.resolution,\n",
    "    center_crop=args.center_crop,\n",
    ")\n",
    "perturbed_wm_data = load_data(\n",
    "    args.wm_instance_data_dir_for_adversarial,\n",
    "    size=args.resolution,\n",
    "    center_crop=args.center_crop,\n",
    ")\n",
    "original_data = perturbed_data.clone()\n",
    "original_data.requires_grad_(False)\n",
    "\n",
    "if args.enable_xformers_memory_efficient_attention:\n",
    "    if is_xformers_available():\n",
    "        unet.enable_xformers_memory_efficient_attention()\n",
    "    else:\n",
    "        raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\n",
    "\n",
    "if args.gradient_checkpointing:\n",
    "        unet.enable_gradient_checkpointing()\n",
    "        if args.train_text_encoder:\n",
    "            text_encoder.gradient_checkpointing_enable()\n",
    "\n",
    "target_latent_tensor = None\n",
    "if args.target_image_path is not None:\n",
    "    target_image_path = Path(args.target_image_path)\n",
    "    assert target_image_path.is_file(), f\"Target image path {target_image_path} does not exist\"\n",
    "\n",
    "    target_image = Image.open(target_image_path).convert(\"RGB\").resize((args.resolution, args.resolution))\n",
    "    target_image = np.array(target_image)[None].transpose(0, 3, 1, 2)\n",
    "\n",
    "    target_image_tensor = torch.from_numpy(target_image).to(\"cuda\", dtype=torch.float32) / 127.5 - 1.0\n",
    "    target_latent_tensor = (\n",
    "        vae.encode(target_image_tensor).latent_dist.sample().to(dtype=torch.bfloat16) * vae.config.scaling_factor\n",
    "    )\n",
    "    target_latent_tensor = target_latent_tensor.repeat(len(perturbed_data), 1, 1, 1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dtype = torch.float16\n",
    "from utils.models import SecretEncoder,SecretDecoder\n",
    "sec_decoder = SecretDecoder(output_size=48).to(accelerator.device, dtype=weight_dtype)\n",
    "models = torch.load('./pretrained_latentwm.pth')\n",
    "sec_decoder.load_state_dict(models['sec_decoder'])\n",
    "sec_decoder.requires_grad_(False)\n",
    "msg_val = torch.tensor(np.load('./secret_48.npy')).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_prior_epoch(\n",
    "    args,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    noise_scheduler,\n",
    "    vae,\n",
    "    text_encoder,\n",
    "    data_tensor: torch.Tensor,\n",
    "    wm_data_tensor:torch.Tensor,\n",
    "    num_steps=20,\n",
    "):\n",
    "     \n",
    "    unet_ori=copy.deepcopy(model[0])\n",
    "    # unet, text_encoder = copy.deepcopy(models[0]), copy.deepcopy(models[1])\n",
    "    unet=copy.deepcopy(model[0])\n",
    "    \n",
    "    del model[0]\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    unet.requires_grad_(True)\n",
    "    # params_to_optimize = itertools.chain(unet.parameters(), text_encoder.parameters())\n",
    "    params_to_optimize = itertools.chain(unet.parameters())\n",
    "\n",
    "    if args.use_8bit_adam:\n",
    "        try:\n",
    "            import bitsandbytes as bnb\n",
    "            optimizer_class = bnb.optim.AdamW8bit\n",
    "        except ImportError:\n",
    "            print(\"bitsandbytes not installed. Falling back to default Adam.\")\n",
    "            optimizer_class = torch.optim.AdamW\n",
    "    else:\n",
    "        optimizer_class = torch.optim.AdamW\n",
    "    optimizer = optimizer_class(\n",
    "        params_to_optimize,\n",
    "        lr=args.learning_rate,\n",
    "        betas=(args.adam_beta1, args.adam_beta2),\n",
    "        weight_decay=args.adam_weight_decay,\n",
    "        eps=args.adam_epsilon,\n",
    "    )\n",
    "\n",
    "    train_dataset = DreamBoothDatasetFromPriorTensor(\n",
    "        data_tensor,\n",
    "        wm_data_tensor,\n",
    "        args.instance_prompt,\n",
    "        tokenizer,\n",
    "        args.class_data_dir,\n",
    "        args.class_prompt,\n",
    "        args.resolution,\n",
    "        args.center_crop,\n",
    "    )\n",
    "\n",
    "    weight_dtype = torch.bfloat16\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    vae.to(device, dtype=weight_dtype)\n",
    "    text_encoder.to(device, dtype=weight_dtype)\n",
    "    unet.to(device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    for step in (range(num_steps)):\n",
    "        unet.train()\n",
    "        \n",
    "        from utils.misc import torch_to_pil\n",
    "        \n",
    "        step_data = train_dataset[step % len(train_dataset)]\n",
    "        pixel_values = torch.stack([step_data[\"wm_instance_images\"], step_data[\"class_images\"]]).to(\n",
    "            device, dtype=weight_dtype\n",
    "        )\n",
    "        ori_values=step_data[\"instance_images\"].to(\n",
    "            device, dtype=weight_dtype\n",
    "        )\n",
    "        # torch_to_pil(pixel_values)[0].show()\n",
    "        # torch_to_pil(ori_values)[0].show()\n",
    "        # print(pixel_values.shape)\n",
    "        # torch_to_pil(pixel_values)[0].show()\n",
    "        # print(step_data[\"instance_prompt_ids\"], step_data[\"class_prompt_ids\"])\n",
    "        \n",
    "        input_ids = torch.cat([step_data[\"instance_prompt_ids\"], step_data[\"class_prompt_ids\"]], dim=0).to(device)\n",
    "\n",
    "        latents = vae.encode(pixel_values).latent_dist.sample()\n",
    "        latents = latents * vae.config.scaling_factor\n",
    "\n",
    "        latents_ori = vae.encode(ori_values.unsqueeze(0)).latent_dist.sample()\n",
    "        latents_ori = latents_ori.repeat(2, 1, 1, 1) * vae.config.scaling_factor\n",
    "        # Sample noise that we'll add to the latents\n",
    "        noise = torch.randn_like(latents)\n",
    "        bsz = latents.shape[0]\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "        timesteps = timesteps.long()\n",
    "\n",
    "        # Add noise to the latents according to the noise magnitude at each timestep\n",
    "        # (this is the forward diffusion process)\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "        noisy_latents_ori=noise_scheduler.add_noise(latents_ori, noise, timesteps)\n",
    "        # Get the text embedding for conditioning\n",
    "        encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "\n",
    "        # Predict the noise residual\n",
    "\n",
    "        model_pred = unet(noisy_latents.to(unet.dtype), timesteps.to(unet.dtype), encoder_hidden_states.to(unet.dtype)).sample\n",
    "        with torch.no_grad():\n",
    "            model_pred_ori = unet_ori(noisy_latents_ori.to(unet_ori.dtype), timesteps, encoder_hidden_states.to(unet_ori.dtype)).sample\n",
    "            \n",
    "            \n",
    "            \n",
    "        # Get the target for loss depending on the prediction type\n",
    "        if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "            target = noise\n",
    "        elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "            target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "        if args.with_prior_preservation:\n",
    "            # Chunk the noise and model_pred into two parts and compute the loss on each part separately.\n",
    "            model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)\n",
    "            target, target_prior = torch.chunk(target, 2, dim=0)\n",
    "\n",
    "            # Compute instance loss\n",
    "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "\n",
    "            # Compute prior loss\n",
    "            prior_loss = F.mse_loss(model_pred_prior.float(), target_prior.float(), reduction=\"mean\")\n",
    "\n",
    "        else:\n",
    "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "            \n",
    "        wm_model_pred, wm_model_pred_prior=torch.chunk(model_pred_ori, 2, dim=0)\n",
    "        \n",
    "        loss_pred=F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "        loss_priorwm=F.mse_loss(model_pred.float(), wm_model_pred.float(), reduction=\"mean\")\n",
    "        loss=loss_priorwm+args.prior_loss_weight*prior_loss\n",
    "\n",
    "        accelerator.backward(loss)\n",
    "        if accelerator.sync_gradients:\n",
    "            params_to_clip = (\n",
    "                itertools.chain(unet.parameters(), text_encoder.parameters())\n",
    "                if args.train_text_encoder\n",
    "                else unet.parameters()\n",
    "            )\n",
    "            accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "    del unet_ori\n",
    "    torch.cuda.empty_cache()\n",
    "    logs = {\"loss\": loss.detach().item()}\n",
    "        \n",
    "    return [unet,logs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = [unet]\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "progress_bar = tqdm(\n",
    "    range(0, args.max_train_steps),\n",
    "    disable=not accelerator.is_local_main_process,\n",
    ")\n",
    "progress_bar.set_description(\"Steps\")\n",
    "for i in (range(args.max_train_steps)):\n",
    "    \n",
    "    # f_sur = train_one_epoch(\n",
    "    #     args,\n",
    "    #     f,\n",
    "    #     tokenizer,\n",
    "    #     noise_scheduler,\n",
    "    #     vae,\n",
    "    #     text_encoder,\n",
    "    #     clean_data,\n",
    "    #     args.max_f_train_steps,\n",
    "    # )\n",
    "    f_sur = train_one_epoch(\n",
    "        args,\n",
    "        f,\n",
    "        tokenizer,\n",
    "        noise_scheduler,\n",
    "        vae,\n",
    "        text_encoder,\n",
    "        original_data,\n",
    "        args.max_f_train_steps,\n",
    "    )\n",
    "    f_sur[0].requires_grad_(False)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect() \n",
    "    \n",
    "    if i<20:\n",
    "        f=f_sur\n",
    "    else:\n",
    "        perturbed_data = perturbation_modualtion(\n",
    "            args,\n",
    "            f_sur,\n",
    "            tokenizer,\n",
    "            noise_scheduler,\n",
    "            vae,\n",
    "            text_encoder,\n",
    "            perturbed_data,\n",
    "            original_data,\n",
    "            args.max_adv_train_steps,\n",
    "            wm_tensor=perturbed_wm_data,\n",
    "            sec_decoder=sec_decoder\n",
    "        )\n",
    "        del f_sur[0]\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        f = train_one_prior_epoch(\n",
    "            args,\n",
    "            f,\n",
    "            tokenizer,\n",
    "            noise_scheduler,\n",
    "            vae,\n",
    "            text_encoder,\n",
    "            perturbed_data,\n",
    "            perturbed_wm_data,\n",
    "            args.max_f_train_steps,\n",
    "        )\n",
    "        f[0].requires_grad_(False)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    \n",
    "    progress_bar.set_postfix(**f[1])\n",
    "    progress_bar.update(1)\n",
    "    \n",
    "    if (i + 1) % args.checkpointing_iterations == 0:\n",
    "        save_folder = f\"{args.output_dir}/noise-ckpt/{i+1}\"\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "        noised_imgs = perturbed_data.detach()\n",
    "        img_names = [\n",
    "            str(instance_path).split(\"/\")[-1]\n",
    "            for instance_path in list(Path(args.instance_data_dir_for_adversarial).iterdir())\n",
    "        ]\n",
    "        for img_pixel, img_name in zip(noised_imgs, img_names):\n",
    "            save_path = os.path.join(save_folder, f\"{i+1}_noise_{img_name}\")\n",
    "            Image.fromarray(\n",
    "                (img_pixel * 127.5 + 128).clamp(0, 255).to(torch.uint8).permute(1, 2, 0).cpu().numpy()\n",
    "            ).save(save_path)\n",
    "        print(f\"Saved noise at step {i+1} to {save_folder}\")\n",
    "        \n",
    "        \n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "                save_path = os.path.join(args.output_dir, f\"checkpoint-{i}\")\n",
    "                ckpt_pipeline = DiffusionPipeline.from_pretrained(\n",
    "                    args.pretrained_model_name_or_path,\n",
    "                    unet=accelerator.unwrap_model(f[0]),\n",
    "                    revision=args.revision,\n",
    "                )\n",
    "                ckpt_pipeline.save_pretrained(save_path)\n",
    "                del ckpt_pipeline\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                prompts = args.inference_prompts.split(\";\")\n",
    "                infer(save_path, prompts, n_img=4, bs=4, n_steps=100,sec_decoder=sec_decoder,msg_val=msg_val)\n",
    "                logger.info(f\"Saved state to {save_path}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watermark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
