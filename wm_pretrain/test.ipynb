{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def load_image(p):\n",
    "    return Image.open(p).convert('RGB').resize((512,512))\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Inference\")\n",
    "parser.add_argument(\n",
    "    \"--model_path\",\n",
    "    type=str,\n",
    "    default='./dreambooth-outputs/concept-n161/checkpoint-1000/',\n",
    "    help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--img_dir\",\n",
    "    type=str,\n",
    "    default=\"./wm_images/images\",\n",
    "    help=\"The output directory where predictions are saved\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--output_dir\",\n",
    "    type=str,\n",
    "    default=\"./wm_images/wm_images\",\n",
    "    help=\"The output directory where predictions are saved\",\n",
    ")\n",
    "\n",
    "args =parser.parse_known_args()[0]\n",
    "\n",
    "image_files = [f for f in os.listdir(args.img_dir) if f.endswith('.png')]\n",
    "images = []\n",
    "for image_file in image_files:\n",
    "    image_path = os.path.join(args.img_dir, image_file)\n",
    "    image = load_image(image_path)\n",
    "    images.append(image)\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "sys.path.append(\"../\")\n",
    "from utils.models import SecretEncoder,SecretDecoder\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    DDPMScheduler,\n",
    "    DiffusionPipeline,\n",
    "    UNet2DConditionModel,\n",
    ")\n",
    "from PIL import Image\n",
    "import random\n",
    "import numpy as np\n",
    "def torch_to_pil(images):\n",
    "    images = images.detach().cpu().float()\n",
    "    if images.ndim == 3:\n",
    "        images = images[None, ...]\n",
    "    images = images.permute(0, 2, 3, 1)\n",
    "    images = (images + 1) * 0.5\n",
    "    images = (images * 255).clamp(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
    "    if images.shape[-1] == 1:\n",
    "        # special case for grayscale (single channel) images\n",
    "        pil_images = [Image.fromarray(image.squeeze(), mode=\"L\") for image in images]\n",
    "    else:\n",
    "        pil_images = [Image.fromarray(image) for image in images]\n",
    "\n",
    "    return pil_images\n",
    "\n",
    "def decode_latents(latents):\n",
    "    # latents = 1 / vae.config.scaling_factor * latents\n",
    "    image = vae.decode(latents).sample\n",
    "    return image\n",
    "\n",
    "def generate_random_mask(batch_size, height=512, width=512):\n",
    "    mask = torch.zeros(batch_size, 3, height, width)\n",
    "    total_area = height * width\n",
    "    for b in range(batch_size):\n",
    "        area_ratio = random.uniform(0.2, 0.2)\n",
    "        target_area = int(total_area * area_ratio)\n",
    "        found = False\n",
    "        while not found:\n",
    "            mask_height = random.randint(1, height)\n",
    "            mask_width = target_area // mask_height\n",
    "            if mask_width <= width and mask_height <= height:\n",
    "                found = True\n",
    "        start_x = random.randint(0, width - mask_width)\n",
    "        start_y = random.randint(0, height - mask_height)\n",
    "        mask[b, :, start_y:start_y + mask_height, start_x:start_x + mask_width] = 1\n",
    "    return mask\n",
    "\n",
    "sec_encoder = SecretEncoder(48).cuda()\n",
    "sec_decoder = SecretDecoder(output_size=48).cuda()\n",
    "models = torch.load('./checkpoints_48bit/checkpoints/state_dict_46.pth')\n",
    "sec_encoder.load_state_dict(models['sec_encoder'])\n",
    "sec_decoder.load_state_dict(models['sec_decoder'])\n",
    "sec_encoder.eval()\n",
    "sec_decoder.eval()\n",
    "from torchvision import transforms\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.CenterCrop(512),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "    '../stable-diffusion-2-1-base/', subfolder=\"vae\").cuda()\n",
    "    \n",
    "    image = Image.open('../1.png')\n",
    "    image=train_transforms(image).cuda()\n",
    "    \n",
    "    mask_tensor = generate_random_mask(1).cuda()\n",
    "    masked_image = image * mask_tensor\n",
    "    \n",
    "    \n",
    "    y_indices, x_indices = torch.where(mask_tensor[0][0] > 0) \n",
    "    if y_indices.numel() == 0 or x_indices.numel() == 0:\n",
    "        raise ValueError(\"Mask must have at least one non-zero element.\")\n",
    "    y_min, y_max = y_indices.min(), y_indices.max() \n",
    "    x_min, x_max = x_indices.min(), x_indices.max()  \n",
    "    cropped_image = image[:, y_min:y_max + 1, x_min:x_max + 1]\n",
    "    target_size = (512, 512) \n",
    "    scaled_image = F.interpolate(cropped_image.unsqueeze(0), size=target_size, mode='bilinear', align_corners=False)\n",
    "    scaled_image = scaled_image\n",
    "\n",
    "    # masked_image=scaled_image\n",
    "    print(scaled_image.shape)\n",
    "    \n",
    "    watermarked_image_pil = torch_to_pil(masked_image)[0]\n",
    "    watermarked_image_pil.show()\n",
    "    watermarked_image_pil = torch_to_pil(scaled_image)[0]\n",
    "    watermarked_image_pil.show()\n",
    "    \n",
    "    latents = vae.encode(masked_image).latent_dist.sample().detach()\n",
    "    \n",
    "    \n",
    "    \n",
    "    msg_val = torch.randint(0, 2, (1, 48)).cuda()\n",
    "    latents=vae.encode(scaled_image).latent_dist.sample().detach()\n",
    "    watermarked_latent, _ = sec_encoder(latents, msg_val.float())\n",
    "    watermarked = decode_latents(watermarked_latent)\n",
    "    \n",
    "    watermarked_image_pil = torch_to_pil(watermarked)[0]\n",
    "    watermarked_image_pil.show()\n",
    "    \n",
    "    decoded_msg = sec_decoder(watermarked* mask_tensor)\n",
    "    decoded_msg = torch.argmax(decoded_msg, dim=2)\n",
    "    acc = 1 - torch.abs(decoded_msg - msg_val).sum().float() / (48 * 1)\n",
    "    print(f\"acc {acc}\")\n",
    "    \n",
    "    \n",
    "    latents = vae.encode(masked_image).latent_dist.sample().detach()\n",
    "    msg_val = torch.randint(0, 2, (1, 48)).cuda()\n",
    "    watermarked_latent, _ = sec_encoder(latents, msg_val.float())\n",
    "    watermarked = decode_latents(watermarked_latent)\n",
    "    watermarked_image_pil = torch_to_pil(watermarked)[0]\n",
    "    watermarked_image_pil.show()\n",
    "    decoded_msg = sec_decoder(watermarked*mask_tensor)\n",
    "    decoded_msg = torch.argmax(decoded_msg, dim=2)\n",
    "    acc = 1 - torch.abs(decoded_msg - msg_val).sum().float() / (48 * 1)\n",
    "    print(f\"acc {acc}\")\n",
    "    exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watermark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
